package ru.yandex.market.core.database;

import java.io.IOException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.text.MessageFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import java.util.SortedSet;
import java.util.concurrent.ConcurrentSkipListSet;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import javax.annotation.concurrent.NotThreadSafe;

import com.google.common.collect.ImmutableMap;
import liquibase.Liquibase;
import liquibase.change.AbstractSQLChange;
import liquibase.changelog.ChangeSet;
import liquibase.database.Database;
import liquibase.database.jvm.JdbcConnection;
import liquibase.exception.LiquibaseException;
import liquibase.parser.core.sql.SqlChangeLogParser;
import liquibase.resource.ClassLoaderResourceAccessor;
import liquibase.resource.ResourceAccessor;
import liquibase.statement.core.RawSqlStatement;
import liquibase.util.file.FilenameUtils;
import org.apache.commons.lang3.ArrayUtils;
import org.apache.commons.lang3.RegExUtils;
import org.apache.commons.lang3.StringUtils;
import org.apache.commons.lang3.Validate;
import org.assertj.core.util.Sets;
import org.springframework.jdbc.core.JdbcTemplate;

import ru.yandex.common.util.collections.Pair;
import ru.yandex.common.util.id.HasId;

@NotThreadSafe
public final class PostgreMigrationsValidator {
    private static final int PG_OBJECT_NAME_LIMIT = 63;
    /**
     * @see SqlChangeLogParser#parse
     */
    private static final String CHANGESET_EMPTY_AUTHOR = "includeAll";
    private static final String CHANGESET_EMPTY_ID = "raw";
    private static final String IGNORED_SCHEMAS_IN = Stream.of(
            "information_schema",
            "pg_catalog",
            "public"
    ).collect(Collectors.joining("','", "('", "')"));
    private static final String C_TYPES = "('r', 'm', 'p')"; // regular table, mat view, partitioned table"
    private static final String IX_TYPES = "('i', 'I')"; // index, partitioned index
    private static final Pattern FILE_NAME_REGULAR = Pattern.compile("[a-z_0-9$]+\\.(?:sql|xml)");
    private static final Pattern FILE_NAME_QUEUE_TICKET = Pattern.compile("[A-Z]+-[0-9]+\\.(?:sql|xml)");
    private static final List<String> NAMING_PATTERNS_FOR_BOOLEAN = List.of(
            "is",
            "does",
            "should",
            "has"
    );
    private static final List<String> NAMING_PATTERNS_FOR_BIGINT = List.of(
            "id",
            "type"
    );
    private static final Map<String, String> FORBIDDEN_DATA_TYPES = ImmutableMap.<String, String>builder()
            .put("char", "use text or varchar(n) if you REALLY need a limit")
            .put("date", "use timestamp with time zone")
            .put("timestamp without time zone", "use timestamp with time zone")
            .build();
    private static final Map<Pattern, String> FORBIDDEN_SQL_PATTERNS = ImmutableMap.<Pattern, String>builder()
            .put(
                    createPattern(" generated (?:always|by default)? as identity\\s*"),
                    "Don't use autogenerated sequences, create them explicitly"
            )
            .put(
                    // для простоты, чтобы не было разброда и шатаний
                    createPattern("\\w+ (?:big|small)?+serial\\d*[\\s,-]+"),
                    "Forbidden '*serial*' datatype, use smallint/int/bigint instead"
            )
            .put(
                    createPattern(" localtimestamp\\s*"),
                    "Don't use localtimestamp, use current_timestamp instead"
            )
            .put(
                    // TODO поправить рекомендацию на boolean после переезда на PG
                    createPattern(String.format(
                            // пропускаем все поля с явным указанием prescision
                            // language=regexp
                            "\\b(?:[\\w_]+_)?(?:%s)(?:_[\\w_]+)?\\b numeric[^(]",
                            String.join("|", NAMING_PATTERNS_FOR_BOOLEAN)
                    )),
                    "Replace numeric to smallint in columns with words " +
                            String.join("/", NAMING_PATTERNS_FOR_BOOLEAN)
            )
            .put(
                    createPattern(String.format(
                            // пропускаем все поля с явным указанием prescision
                            // language=regexp
                            "\\b(?:[\\w_]+_)?(?:%s)(?:_[\\w_]+)?\\b numeric[^(]",
                            String.join("|", NAMING_PATTERNS_FOR_BIGINT)
                    )),
                    "Replace numeric to bigint in columns with words " +
                            String.join("/", NAMING_PATTERNS_FOR_BIGINT)
            )
            .put(
                    // у now() также нельзя указать точность в отличие от current_timestamp
                    createPattern(" now\\(\\s*\\)\\s*"),
                    "Use current_timestamp instead of now() for uniformity"
            )
            .put(
                    // процедуры в отличие от функций также например могут вызывать commit транзакции
                    // https://www.postgresql.org/docs/13/plpgsql-transactions.html
                    createPattern("returns void"),
                    "Declare procedure instead of void function"
            )
            .put(
                    createPattern("drop .+cascade"),
                    "Forbidden cascade deletion"
            )
            //put new patterns created with createPattern() func
            .build();
    private static final Set<String> ALLOWED_CHANGE_SET_PATHS = Stream.concat(
            Stream.of(
                    "schemas/",
                    "tables/",
                    "sequences/",
                    "views/",
                    "procedures/",
                    "compat/ora/",
                    "compat/ora2pg/"
            ).flatMap(dir -> Stream.of(
                    "create/" + dir,
                    "remove/" + dir
            )),
            Stream.of("update/")
    ).collect(Collectors.toUnmodifiableSet());

    // see https://wiki.yandex-team.ru/users/sergeyvalkov/liquibase-changelog-organization/#examples
    private static final Map<String, Set<Pattern>> ALLOWED_PATTERNS_FOR_DIRECTORY = Map.of(
            "/create/tables/", Stream.of(
                    "alter table ",
                    "comment on ",
                    "create table ",
                    // https://medium.com/flatiron-engineering/uniqueness-in-postgresql-constraints-versus-indexes-4cf957a472fd
                    // Use 'alter table add constraint ... unique' instead of 'create unique index'
                    "create index ",
                    "create unique index[^(]+\\([^)]*\\(",
                    "delete from ", // eg to delete duplicates before adding constraint
                    "drop index ", // see wiki for explanation
                    "drop view "
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet()),
            "/create/sequences/", Stream.of(
                    "comment on ",
                    "create sequence "
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet()),
            "/create/schemas/", Stream.of(
                    "comment on ",
                    "create schema "
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet()),
            "/create/procedures/", Stream.of(
                    "comment on ",
                    "create (?:or replace )?(?:function|procedure|trigger) ",
                    "create type ",
                    "create cast "
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet()),
            "/create/views/", Stream.of(
                    "comment on ",
                    "create (?:or replace )?view ",
                    "create (?:materialized )?view ",
                    "create (?:unique )?index ",
                    "drop index ", // see wiki for explanation
                    "drop (?:materialized )?view ", // see wiki for explanation
                    "insert into monitor.monitoring " // clear way to add new monitoring, right after his view
                    // (https://wiki.yandex-team.ru/users/sergeyvalkov/liquibase-changelog-organization/#izmenenievjuxi)
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet()),
            "/update/", Stream.of(
                    "update ",
                    "delete from ",
                    "insert "
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet()),
            "/remove/", Stream.of(
                    "drop "
            ).map(PostgreMigrationsValidator::createPattern).collect(Collectors.toSet())
    );
    private static final Pattern PLPGSQL_SPLIT = Pattern.compile("\\$\\$(?:\\s+language\\s+plpgsql[^;]*)?;");

    private static final Set<String> PATTERN_FOR_DIRECTORY_ASSUMPTIONS = Set.of(
            "pg/create/tables/mbi_core_qrtz_log.sql", //func
            "pg/create/tables/market_billing_mst_rcv_queue.sql", //type
            "pg/create/tables/shops_web_papi_market_sku_offer.sql", //func, trigger
            "pg/create/views/shops_web_v_agency_campaigns.sql", //type
            "pg/create/views/market_billing_campaign_balance_details_view.sql", //func
            "assumptions-end" // tail to reduce delta
    );

    private static final Pattern NAME_FROM_REGEXP_OPTS = Pattern.compile("\\(\\?[^)]+\\)\\?");
    private static final Pattern NAME_FROM_REGEXP = Pattern.compile("(?:\\[a-z0-9]\\*)*[|()?:]*(?:\\{[0-9]*})*");
    private static final Pattern WORDS_SEPARATORS = Pattern.compile("[ .,\n]+");
    private static final Pattern INDEX_COLUMNS_SEPARATOR = Pattern.compile("\\s*,\\s*");
    private static final String FORMAT_ERROR_MESSAGE = "Wrong {0} name [%s] in table [%s.%s], try eg [%s] instead";
    private static final String WRONG_PRIMARY_KEY_NAME = MessageFormat.format(
            FORMAT_ERROR_MESSAGE, "primary key"
    );
    private static final String WRONG_CHECK_NAME = MessageFormat.format(
            FORMAT_ERROR_MESSAGE, "check"
    );
    private static final String WRONG_FOREIGN_KEY_NAME = MessageFormat.format(
            FORMAT_ERROR_MESSAGE, "foreign key"
    );
    private static final String WRONG_INDEX_NAME = MessageFormat.format(
            FORMAT_ERROR_MESSAGE, "index"
    );
    private static final String LINK =
            "https://wiki.yandex-team.ru/users/sergeyvalkov/liquibase-changelog-organization/";

    private final JdbcTemplate jdbcTemplate;
    private final Set<String> changeLogPaths;

    private final SortedSet<String> errors = new ConcurrentSkipListSet<>();
    private final Set<ChangeSet> uniqueChangeSets = new HashSet<>();
    private int schemasCount;
    private Set<String> scriptFileNames;

    public static Optional<String> validate(
            JdbcTemplate jdbcTemplate,
            Collection<String> changeLogPaths
    ) {
        return new PostgreMigrationsValidator(jdbcTemplate, changeLogPaths).validate();
    }

    private PostgreMigrationsValidator(
            JdbcTemplate jdbcTemplate,
            Collection<String> changeLogPaths
    ) {
        this.jdbcTemplate = jdbcTemplate;
        this.changeLogPaths = Set.copyOf(changeLogPaths);
    }

    private Set<String> getAllScripts(ResourceAccessor fileOpener) {
        try {
            return fileOpener.list("", "pg", true, false, true)
                    .stream().filter(path -> path.endsWith(".sql"))
                    .map(x -> StringUtils.substringAfterLast(x, "/")).collect(Collectors.toSet());
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    private static Pattern createPattern(String regex) {
        return Pattern.compile(
                regex.replaceAll("\\s+", "\\\\s+"), // чтобы проще было писать пробельчики
                Pattern.CASE_INSENSITIVE | Pattern.DOTALL
        );
    }

    private static String makeNameFromRegex(String regex) {
        var withoutOptionalParts = RegExUtils.removeAll(regex, NAME_FROM_REGEXP_OPTS);
        return RegExUtils.removeAll(withoutOptionalParts, NAME_FROM_REGEXP);
    }

    private static String getFirstNLettersOrLess(String str, int letters) {
        return StringUtils.substring(str, 0, letters).toLowerCase();
    }

    private static Set<String> getSet(ResultSet rs, String columnName) throws SQLException {
        // preserve elements order
        return Collections.unmodifiableSet(Sets.newLinkedHashSet((String[]) rs.getArray(columnName).getArray()));
    }

    private static List<String> getList(ResultSet rs, String columnName) throws SQLException {
        return List.of((String[]) rs.getArray(columnName).getArray());
    }

    /**
     * @return error message if any or {@link Optional#empty()} otherwise
     */
    private Optional<String> validate() {
        errors.clear();
        uniqueChangeSets.clear();
        schemasCount = getSchemasCount();

        validate(validationTableAndDataTypes());
        validate(validationConstraints());
        validate(validationForeignKeys());
        validate(validationIndexes());
        validate(validationChangeSets());
        validateIndexesWithNumericFields();
        validateIndexesForFks();
        validateMultipleIndexesOnSameColumns();
        validateNumericToInts();

        return errors.isEmpty()
                ? Optional.empty()
                : Optional.of("Incorrect migrations, see " + LINK + "\n - " + String.join("\n - ", errors));
    }

    private static void validate(Collection<Runnable> validations) {
        for (var validation : validations) {
            validation.run();
        }
    }

    private void validateDirectoryStructure(ChangeSet changeSet) {
        var filePath = FilenameUtils.getPath(changeSet.getFilePath());
        if (ALLOWED_CHANGE_SET_PATHS.stream().noneMatch(filePath::endsWith)) {
            errors.add(String.format("Forbidden changeSet path [%s]", changeSet.getFilePath()));
        }
    }

    private void removeFromUselessList(ChangeSet changeSet) {
        var name = changeSet.getFilePath().split("/");
        scriptFileNames.remove(name[name.length - 1]);
    }

    private void validateForbiddenSqlPatterns(ChangeSet changeSet) {
        changeSet.getChanges().forEach(change -> {
            if (change instanceof AbstractSQLChange) {
                var sqlChange = (AbstractSQLChange) change;
                FORBIDDEN_SQL_PATTERNS.forEach((forbiddenPattern, errorMsg) -> {
                    var matcher = forbiddenPattern.matcher(sqlChange.getSql());
                    if (matcher.find()) {
                        errors.add(String.format(
                                "[%s] %s. Offending match [%s]",
                                changeSet.getFilePath(), errorMsg, matcher.group()
                        ));
                    }
                });
            }
        });
    }

    private void validateWordLengthInChangeSet(ChangeSet changeSet) {
        // trim comments
        changeSet.getChanges().parallelStream()
                .filter(change -> change instanceof AbstractSQLChange)
                .map(AbstractSQLChange.class::cast)
                .flatMap(sqlChange -> Stream.of(StringUtils.split(sqlChange.getSql(), "")))
                .map(line -> StringUtils.substringBefore(line, "--")) // trim comments
                .flatMap(line -> Stream.of(WORDS_SEPARATORS.split(line)))
                .filter(word -> !word.contains("'"))
                .filter(word -> word.trim().length() > PG_OBJECT_NAME_LIMIT)
                .map(word -> String.format(
                        "There is word [%s] larger than %d symbols limit in [%s]",
                        word.trim(),
                        PG_OBJECT_NAME_LIMIT,
                        changeSet.getFilePath()
                ))
                .forEach(errors::add);
    }

    /**
     * @see ChangeSet#id
     */
    private void validateChangeSetId(ChangeSet changeSet) {
        if (!uniqueChangeSets.add(changeSet)) {
            errors.add(String.format("Duplicate changeSet author:id in [%s]", changeSet));
        }
        if (CHANGESET_EMPTY_ID.equalsIgnoreCase(changeSet.getId())) {
            errors.add(String.format("Empty changeSet id in [%s]", changeSet));
        }
        if (CHANGESET_EMPTY_AUTHOR.equalsIgnoreCase(changeSet.getAuthor())) {
            errors.add(String.format("Empty changeSet author in [%s]", changeSet));
        }
    }

    private void validateSqlMigrationFileName(ChangeSet changeSet) {
        var fileName = FilenameUtils.getName(changeSet.getFilePath());
        if (changeSet.getFilePath().contains("/update/")) {
            if (!FILE_NAME_QUEUE_TICKET.matcher(fileName).matches()) {
                errors.add(String.format(
                        "Wrong filename [%s], should be /update/QUEUENAME-TICKETNUMBER.sql",
                        changeSet.getFilePath()
                ));
            }
        } else if (!FILE_NAME_REGULAR.matcher(fileName).matches()) {
            errors.add(String.format(
                    "Wrong filename [%s], should match [%s]",
                    changeSet.getFilePath(), FILE_NAME_REGULAR
            ));
        }
    }

    private void validateViewCreation(ChangeSet changeSet, Database database) {
        if (!changeSet.getFilePath().contains("/create/views/")) {
            return;
        }
        List<String> changes = changeSet.getChanges().stream()
                .filter(change -> change instanceof AbstractSQLChange)
                .map(AbstractSQLChange.class::cast)
                .flatMap(sqlChange -> {
                    sqlChange.setSplitStatements(true);
                    sqlChange.setStripComments(true);
                    return Stream.of(sqlChange.generateStatements(database));
                })
                .map(RawSqlStatement.class::cast)
                .flatMap(sqlStatement -> ";".equals(sqlStatement.getEndDelimiter())
                        ? Stream.of(sqlStatement.getSql())
                        : Stream.of(PLPGSQL_SPLIT.split(sqlStatement.getSql())))
                .filter(sql -> sql.contains(" view ") && !sql.contains("comment on")) //drop/create view with all additions
                .collect(Collectors.toList());
        if (!changes.isEmpty() && !changes.get(changes.size() - 1).startsWith("create")) {
            errors.add(String.format(
                    "Last change in [%s] isn't [create view] in [/create/view/] file.",
                    changeSet.getFilePath()
            ));
        }
    }

    private void validateIndexCreation(ChangeSet changeSet) {
        if (changeSet.getChanges().parallelStream()
                .filter(change -> change instanceof AbstractSQLChange)
                .map(AbstractSQLChange.class::cast)
                .map(AbstractSQLChange::getSql)
                .filter(sql -> StringUtils.countMatches(sql, "create index") //drop where only concurrently
                        != StringUtils.countMatches(sql, "create index concurrently")
                        || StringUtils.countMatches(sql, "create unique index")
                        != StringUtils.countMatches(sql, "create unique index concurrently"))
                .anyMatch(sqlChange -> (sqlChange.contains("create index") || sqlChange.contains("create unique index"))
                        && !(sqlChange.contains("create table") || sqlChange.contains("create materialized view"))
                )
        ) {
            errors.add(String.format("Wrong index creation in [%s]. Use [create index concurrently] and add " +
                    "[runInTransaction:false] after changeSet name", changeSet.getFilePath()));
        }
    }

    private void validatePathToContent(ChangeSet changeSet, Database database) {
        Optional<String> directory = ALLOWED_PATTERNS_FOR_DIRECTORY.keySet().stream()
                .filter(changeSet.getFilePath()::contains)
                .findFirst();
        if (directory.isEmpty() || PATTERN_FOR_DIRECTORY_ASSUMPTIONS.contains(changeSet.getFilePath())) {
            return; //forbidden changeset path
        }

        var allowedPatterns = ALLOWED_PATTERNS_FOR_DIRECTORY.get(directory.get());
        changeSet.getChanges().stream()
                .filter(change -> change instanceof AbstractSQLChange)
                .map(AbstractSQLChange.class::cast)
                .flatMap(sqlChange -> {
                    sqlChange.setSplitStatements(true);
                    sqlChange.setStripComments(true);
                    return Stream.of(sqlChange.generateStatements(database));
                })
                .flatMap(sqlStatement -> {
                    var rawSqlStatement = (RawSqlStatement) sqlStatement;
                    return ";".equals(rawSqlStatement.getEndDelimiter())
                            ? Stream.of(rawSqlStatement.getSql())
                            : Stream.of(PLPGSQL_SPLIT.split(rawSqlStatement.getSql()));
                })
                .filter(sqlExpression -> allowedPatterns.stream()
                        .noneMatch(pattern -> pattern.matcher(sqlExpression).find())
                )
                .forEach(sqlExpression -> errors.add(String.format(
                        "Sql expression [%s] from [%s] not allowed for directory [%s]",
                        sqlExpression.replaceAll("\\s+", " "),
                        changeSet.getFilePath(),
                        directory.get()
                )));
    }

    private List<Runnable> validationChangeSets() {
        var fileOpener = new ClassLoaderResourceAccessor();
        scriptFileNames = getAllScripts(fileOpener);
        return changeLogPaths.stream().map(changeLogPath -> (Runnable) () -> jdbcTemplate.execute((Connection con) -> {
            try {
                var liquibase = new Liquibase(changeLogPath, fileOpener, new JdbcConnection(con));
                var changeSets = liquibase.getDatabaseChangeLog().getChangeSets();
                for (var changeSet : changeSets) {
                    validateChangeSetId(changeSet);
                    validateSqlMigrationFileName(changeSet);
                    validateWordLengthInChangeSet(changeSet);
                    validateForbiddenSqlPatterns(changeSet);
                    validateDirectoryStructure(changeSet);
                    validatePathToContent(changeSet, liquibase.getDatabase());
                    validateIndexCreation(changeSet);
                    validateViewCreation(changeSet, liquibase.getDatabase());
                    removeFromUselessList(changeSet);
                }
                validateUselessFiles();
                return null;
            } catch (LiquibaseException e) {
                throw new RuntimeException(e);
            }
        })).collect(Collectors.toList());
    }

    private void validateUselessFiles() {
        if (!scriptFileNames.isEmpty()) {
            errors.add("Found unused scripts: " + scriptFileNames);
        }
    }

    private int getSchemasCount() {
        return Objects.requireNonNull(jdbcTemplate.queryForObject(
                "select count(schema_name) " +
                        "from information_schema.schemata " +
                        "where schema_name not like 'pg_%' " +
                        "  and schema_name not in " + IGNORED_SCHEMAS_IN,
                Integer.class));
    }

    private List<Runnable> validationTableAndDataTypes() {
        var exceptions = Set.of(
                "market_billing.accrual",
                "market_billing.storage_billing",
                "market_billing.v_storage_billing",
                "market_billing.v_yt_exp_storage_billing",
                "shops_web.arp_cutprice_client_info",
                "shops_web.arp_cutprice_datasource_info",
                "shops_web.daily_calendars"
        );

        var tablePrimaryUq = new HashMap<String, String>();
        var tableColumnPrimaryUq = new HashMap<String, String>();
        return jdbcTemplate.query(
                "select concat_ws('.', c.table_schema, c.table_name) as table_name_full," +
                        "   c.column_name," +
                        "   c.data_type," +
                        "   coalesce(c.character_maximum_length, 0) as character_maximum_length," +
                        "   ccu.constraint_name as uq_constraint" +
                        " from information_schema.tables t" +
                        " join information_schema.columns c" +
                        "   on c.table_schema = t.table_schema" +
                        "   and c.table_name = t.table_name" +
                        " left join information_schema.table_constraints as tc" +
                        "   on tc.table_schema = c.table_schema" +
                        "   and tc.table_name = c.table_name" +
                        "   and tc.constraint_type in ('PRIMARY KEY', 'UNIQUE')" +
                        " left join information_schema.constraint_column_usage as ccu" +
                        "   on ccu.table_schema = tc.table_schema" +
                        "   and ccu.constraint_name = tc.constraint_name" +
                        "   and ccu.column_name = c.column_name" +
                        " where t.table_schema not in " + IGNORED_SCHEMAS_IN +
                        "   and t.table_type not in ('VIEW')" +
                        " order by table_name_full, uq_constraint desc nulls last", // so "pk_" goes before "i_"
                (rs, rowNum) -> {
                    var tableName = rs.getString("table_name_full");
                    var columnName = rs.getString("column_name");
                    var dataType = rs.getString("data_type");
                    var cml = rs.getInt("character_maximum_length");
                    var uq = rs.getString("uq_constraint");
                    var tableNameColumn = tableName + ":" + columnName;
                    if (uq != null) {
                        tablePrimaryUq.putIfAbsent(tableName, uq);
                        tableColumnPrimaryUq.putIfAbsent(tableNameColumn, uq);
                    }
                    return () -> {
                        var forbiddenTypeHint = FORBIDDEN_DATA_TYPES.get(dataType);
                        if (forbiddenTypeHint != null) {
                            if (!exceptions.contains(tableName)) {
                                errors.add(String.format(
                                        "There is forbidden datatype [%s] in table [%s], %s",
                                        dataType, tableName, forbiddenTypeHint
                                ));
                            }
                        } else if ("character varying".equals(dataType)) {
                            if (cml >= 2000 && uq == null) {
                                errors.add(String.format(
                                        "[%s] table, change varchar type of [%s] column to text",
                                        tableName, columnName
                                ));
                            }
                        } else if ("text".equals(dataType)) {
                            var uqPrimaryTable = tablePrimaryUq.get(tableName);
                            var uqPrimaryColumn = tableColumnPrimaryUq.get(tableNameColumn);
                            if (uqPrimaryTable != null && uqPrimaryTable.equals(uqPrimaryColumn)) {
                                errors.add(String.format(
                                        "[%s] table, change text type of [%s] column to varchar," +
                                                " it is used in primary uq constraint [%s]",
                                        tableName, columnName, uqPrimaryTable
                                ));
                            }
                        }
                    };
                }
        );
    }

    private List<Runnable> validationIndexes() {
        return jdbcTemplate.query(
                "select n.nspname as table_schema," +
                        "   t.relname as table_name," +
                        "   i.relname as constraint_name," +
                        "   array_agg(a.attname::text) as columns_names" +
                        " from pg_index ix" +
                        "   join pg_class t on t.oid = ix.indrelid" +
                        "   join pg_class i on i.oid = ix.indexrelid" +
                        "   join pg_namespace n on n.oid = t.relnamespace" +
                        "   join pg_attribute a on a.attrelid = ix.indrelid" +
                        " where n.nspname not in " + IGNORED_SCHEMAS_IN +
                        "   and (" +
                        "       a.attnum = any (ix.indkey)" +
                        //      or functional index definition contains attrname
                        "       or pg_get_expr(ix.indexprs, ix.indrelid) ~* ('\\m' || a.attname || '\\M')" +
                        "   )" +
                        "   and not ix.indisprimary" +
                        " group by n.nspname, t.relname, i.relname",
                (rs, rowNum) -> new Constraint(
                        rs.getString("table_schema"),
                        rs.getString("table_name"),
                        rs.getString("constraint_name"),
                        "index",
                        getSet(rs, "columns_names")
                )
        );
    }

    private List<Runnable> validationForeignKeys() {
        return jdbcTemplate.query(
                "select tc.constraint_name," +
                        "    tc.table_schema," +
                        "    tc.table_name," +
                        "    ccu.table_schema as ref_schema_name," +
                        "    ccu.table_name as ref_table_name," +
                        "    array_agg(kcu.column_name::text) as columns_names," +
                        "    array_agg(ccu.column_name::text) as ref_columns" +
                        " from information_schema.table_constraints as tc" +
                        "    join information_schema.key_column_usage as kcu" +
                        "      on tc.constraint_name = kcu.constraint_name" +
                        "      and tc.table_schema = kcu.table_schema" +
                        "    join information_schema.constraint_column_usage as ccu" +
                        "      on ccu.constraint_name = tc.constraint_name" +
                        " where tc.constraint_type = 'FOREIGN KEY'" +
                        "  and tc.table_schema not in " + IGNORED_SCHEMAS_IN +
                        " group by" +
                        "   tc.constraint_name, tc.table_schema, tc.table_name," +
                        "   ccu.table_schema, ccu.table_name",
                (rs, rowNum) -> new ForeignKey(
                        rs.getString("table_schema"),
                        rs.getString("table_name"),
                        rs.getString("ref_schema_name"),
                        rs.getString("ref_table_name"),
                        rs.getString("constraint_name"),
                        getSet(rs, "ref_columns"),
                        getSet(rs, "columns_names")
                )
        );
    }

    private List<Runnable> validationConstraints() {
        return jdbcTemplate.query(
                "select tc.table_schema," +
                        "   tc.table_name," +
                        "   tc.constraint_name," +
                        "   tc.constraint_type," +
                        "   array_agg(ccu.column_name::text) as columns_names" +
                        " from information_schema.table_constraints as tc" +
                        "   join information_schema.constraint_column_usage as ccu" +
                        "       on ccu.constraint_name = tc.constraint_name" +
                        "       and ccu.table_schema = tc.table_schema" +
                        " where tc.table_schema not in " + IGNORED_SCHEMAS_IN +
                        "   and tc.constraint_type not in ('FOREIGN KEY', 'UNIQUE')" +
                        " group by tc.table_schema, tc.table_name, tc.constraint_name, tc.constraint_type",
                (rs, rowNum) -> new Constraint(
                        rs.getString("table_schema"),
                        rs.getString("table_name"),
                        rs.getString("constraint_name"),
                        rs.getString("constraint_type"),
                        getSet(rs, "columns_names")
                )
        );
    }

    private void validateIndexesWithNumericFields() {
        jdbcTemplate.query(
                "select" +
                        "   n.nspname as table_schema," +
                        "   c.relname as table_name," +
                        "   i.relname as constraint_name," +
                        "   string_agg(a.attname::text, ', ') as columns_names" +
                        " from pg_index ix" +
                        "   join pg_class i on i.oid = ix.indexrelid" +
                        "   join pg_class c on c.oid = ix.indrelid" +
                        "   join pg_namespace n on n.oid = c.relnamespace" +
                        "   join pg_attribute a on a.attrelid = ix.indrelid" +
                        "   join information_schema.columns col on col.table_schema = n.nspname" +
                        "       and col.table_name = c.relname" +
                        "       and col.column_name = a.attname" +
                        " where n.nspname not in " + IGNORED_SCHEMAS_IN +
                        "   and c.relkind in " + C_TYPES +
                        "   and i.relkind in " + IX_TYPES +
                        "   and a.attnum = any (ix.indkey[:ix.indnatts-ix.indnkeyatts])" +
                        "   and a.atttypid = 1700" +
                        "   and (col.numeric_scale is null or col.numeric_scale = 0)" +
                        " group by i.relname, n.nspname, c.relname",
                (rs, rowNum) -> errors.add(String.format(
                        "Numeric key columns [%s] in index [%s] for table [%s.%s], use smallint/bigint/int instead",
                        rs.getString("columns_names"),
                        rs.getString("constraint_name"),
                        rs.getString("table_schema"),
                        rs.getString("table_name")
                ))

        );
    }

    private void validateIndexesForFks() {
        jdbcTemplate.query(
                "with fk(schemaname, tablename, condef)" +
                        "         as (select n.nspname as schemaname," +
                        "                    c.relname as tablename," +
                        "                    array_agg(regexp_match(" +
                        "                            pg_get_constraintdef(r.oid, true)," +
                        "                            '(?:foreign key \\()?([a-zA-Z_, ]+)(?:\\))?'," +
                        "                            'i'" +
                        "                        )::text)    as fk_defs" +
                        "             from pg_constraint r" +
                        "                      join pg_class c on c.oid = r.conrelid" +
                        "                      join pg_namespace n on n.oid = c.relnamespace" +
                        "             where r.contype = 'f'" +
                        "               and n.nspname not in " + IGNORED_SCHEMAS_IN +
                        "             group by n.nspname, c.relname)," +
                        "     idx(schemaname, tablename, index_defs)" +
                        "         as (select n.nspname as schemaname," +
                        "                    c.relname as tablename," +
                        "                    array_agg(coalesce(" +
                        "                            regexp_match(" +
                        "                               pg_get_indexdef(ix.indexrelid), '\\(([a-zA-Z_, ]+)\\)'" +
                        "                             )::text," +
                        "                            '{''}'" +
                        "                        ))    as index_defs" +
                        "             from pg_index ix" +
                        "                      join pg_class c on c.oid = ix.indrelid" +
                        "                      join pg_namespace n on n.oid = c.relnamespace" +
                        "             where n.nspname not like 'pg_%'" +
                        "               and n.nspname not in " + IGNORED_SCHEMAS_IN +
                        "               and c.relkind in " + C_TYPES +
                        "             group by n.nspname, c.relname)" +
                        "select fk.schemaname, fk.tablename, fk.condef as fkdef, index_defs " +
                        " from fk left join idx on idx.schemaname = fk.schemaname and fk.tablename = idx.tablename",
                rs -> {
                    String[] foreignKeyDefs = (String[]) rs.getArray("fkdef").getArray();
                    Set<String> indexDefs = Stream.of((String[]) rs.getArray("index_defs").getArray())
                            .map(index -> index.replaceAll("[{\"}]", ""))
                            .collect(Collectors.toSet());
                    String tableName = rs.getString("tablename");
                    String schemaName = rs.getString("schemaname");
                    Arrays.stream(foreignKeyDefs)
                            .map(foreignKey -> foreignKey.replaceAll("[{\"}]", ""))
                            .filter(foreignKey -> indexDefs.stream().noneMatch(index -> index.startsWith(foreignKey)))
                            .forEach(foreignKey -> errors.add(String.format(
                                    "Missing index for table [%s.%s] for fk with columns [%s], performance will suffer",
                                    schemaName,
                                    tableName,
                                    foreignKey
                            )));
                }
        );
    }

    private void validateNumericToInts() {
        jdbcTemplate.query(
                "select tab.table_schema, tab.table_name, col.column_name, col.numeric_precision " +
                        "from information_schema.columns col " +
                        "         join information_schema.tables tab " +
                        "              on table_type = 'BASE TABLE' " +
                        "                  and col.table_name = tab.table_name " +
                        "                  and col.table_schema = tab.table_schema " +
                        "where data_type = 'numeric' " +
                        "  and numeric_precision <= 20 " +
                        "  and numeric_scale = 0",
                rs -> {
                    int precision = Integer.parseInt(rs.getString("numeric_precision"));
                    if (precision <= 4) {
                        errors.add(String.format(
                                "[%s.%s] table, change numeric type of [%s] column to smallint",
                                rs.getString("table_schema"),
                                rs.getString("table_name"),
                                rs.getString("column_name")
                        ));
                    } else if (precision <= 9) {
                        errors.add(String.format(
                                "[%s.%s] table, change numeric type of [%s] column to int",
                                rs.getString("table_schema"),
                                rs.getString("table_name"),
                                rs.getString("column_name")
                        ));
                    } else {
                        errors.add(String.format(
                                "[%s.%s] table, change numeric type of [%s] column to bigint",
                                rs.getString("table_schema"),
                                rs.getString("table_name"),
                                rs.getString("column_name")
                        ));
                    }
                }
        );
    }

    private void validateMultipleIndexesOnSameColumns() {
        // https://www.postgresql.org/docs/current/catalog-pg-class.html
        var braces = new char[]{'(', ')'};
        jdbcTemplate.query(
                "select n.nspname as schemaname," +
                        "   c.relname as tablename," +
                        "   c.relkind as object_type," +
                        "   array_agg(i.relname) as index_names," +
                        "   array_agg(pg_get_indexdef(i.oid)) as index_defs," +
                        "   array_agg(ix.indisprimary) as is_primary_index," +
                        "   array_agg(ix.indisunique) as is_unique_index," +
                        "   array_agg(not exists(" +
                        "       select" +
                        "       from pg_attribute a" +
                        "       where a.attrelid = ix.indrelid" +
                        "         and a.attnum = any (ix.indkey)" +
                        "         and not a.attnotnull" +
                        "   )) as all_cols_not_null" +
                        " from pg_index ix" +
                        "   join pg_class i on i.oid = ix.indexrelid" +
                        "   join pg_class c on c.oid = ix.indrelid" +
                        "   join pg_namespace n on n.oid = c.relnamespace" +
                        " where n.nspname not like 'pg_%'" +
                        "   and n.nspname not in " + IGNORED_SCHEMAS_IN +
                        "   and c.relkind in " + C_TYPES +
                        "   and i.relkind in " + IX_TYPES +
                        " group by n.nspname, c.relname, c.relkind",
                rs -> {
                    var schemaName = rs.getString("schemaname");
                    var tableName = rs.getString("tablename");
                    var objectType = rs.getString("object_type");
                    var indexNames = getList(rs, "index_names");
                    var indexDefs = getList(rs, "index_defs");
                    var isPrimaryIndex = (Boolean[]) rs.getArray("is_primary_index").getArray();
                    var isUniqueIndex = (Boolean[]) rs.getArray("is_unique_index").getArray();
                    var areIndexColsNotNull = (Boolean[]) rs.getArray("all_cols_not_null").getArray();
                    var indexList = new ArrayList<Index>(indexNames.size());
                    for (int i = 0, n = indexNames.size(); i < n; i++) {
                        var indexName = indexNames.get(i).toLowerCase();
                        var indexDef = indexDefs.get(i).toLowerCase();
                        var parsedColumns = nextColumnsGroup(indexDef, 0);
                        var parsedColumnsIncluded = nextColumnsGroup(indexDef, parsedColumns.getSecond());
                        var indexColumns = Stream.concat(
                                Stream.of(parsedColumns.getFirst()),
                                Stream.of(parsedColumnsIncluded.getFirst())
                        ).collect(Collectors.toUnmodifiableList());
                        indexList.add(new Index(
                                indexColumns,
                                indexName,
                                isPrimaryIndex[i],
                                isUniqueIndex[i],
                                indexColumns.stream().anyMatch(c -> StringUtils.containsAny(c, braces)),
                                indexDef.contains(" where "),
                                areIndexColsNotNull[i]
                        ));
                    }

                    // look for intersections and report redundant ones
                    for (int i = 0, n = indexList.size(); i < n; i++) {
                        for (int j = i + 1; j < n; j++) {
                            Index i1 = indexList.get(i);
                            Index i2 = indexList.get(j);
                            Index iredundant = null;
                            var prefix = Math.min(i1.length(), i2.length());
                            if (i1.columns.subList(0, prefix).equals(i2.columns.subList(0, prefix))) {
                                // если нет общего префикса, то идем дальше
                                if (i1.isPreferredOver(i2)) {
                                    iredundant = i2;
                                } else if (i2.isPreferredOver(i1)) {
                                    iredundant = i1;
                                }
                            }
                            if (iredundant != null) {
                                errors.add(String.format(
                                        "Redundant or duplicate index [%s] for [%s.%s] table",
                                        iredundant.name, schemaName, tableName
                                ));
                            }
                        }
                    }

                    // check if there is a single uniq index that should be pk, ignoring mat views
                    if (!"m".equals(objectType) && indexList.stream().noneMatch(i -> i.isPrimary)) {
                        var uq = indexList.stream()
                                .filter(Index::canBeConvertedToPk)
                                .map(i -> i.name)
                                .collect(Collectors.toList());
                        if (uq.size() == 1) {
                            errors.add(String.format(
                                    "Single unique index [%s] for [%s.%s] table should be converted to pk",
                                    uq.get(0), schemaName, tableName
                            ));
                        }
                    }
                }
        );
    }

    private static Pair<String[], Integer> nextColumnsGroup(String indexDef, int fromIndex) {
        var braceOpen = indexDef.indexOf('(', fromIndex);
        if (braceOpen < 0) {
            return Pair.of(ArrayUtils.EMPTY_STRING_ARRAY, braceOpen);
        }
        var braceLevel = 1;
        var braceClose = braceOpen + 1;
        for (int i = braceClose, n = indexDef.length(); i < n && braceLevel > 0; i++) {
            var c = indexDef.charAt(i);
            if (c == '(') {
                braceLevel++;
            } else if (c == ')') {
                braceClose = i;
                braceLevel--;
            }
        }
        Validate.validState(braceLevel == 0, "No closing brace for index [%s]", indexDef);
        var parsedColumns = indexDef.substring(braceOpen + 1, braceClose);
        return Pair.of(INDEX_COLUMNS_SEPARATOR.split(parsedColumns), braceClose + 1);
    }

    private static class Index {
        final List<String> columns;
        final String name;
        final boolean isPrimary;
        final boolean isUnique;
        final boolean isFunctional;
        final boolean isPartial;
        final boolean areIndexColsNotNull;

        Index(
                List<String> columns,
                String name,
                boolean isPrimary,
                boolean isUnique,
                boolean isFunctional,
                boolean isPartial,
                boolean areIndexColsNotNull
        ) {
            this.columns = columns;
            this.name = name;
            this.isPrimary = isPrimary;
            this.isUnique = isUnique;
            this.isFunctional = isFunctional;
            this.isPartial = isPartial;
            this.areIndexColsNotNull = areIndexColsNotNull;
        }

        private int length() {
            return columns.size();
        }

        /**
         * если unique частично дублирует pk - это ок
         * если обычный частично дублирует уникальный - это ок
         * pk.length >= unique.length >= regular.length >= regular.length = error
         * <p>
         * хотя и можно было бы расширить pk или unique через INCLUDE (columns...) начиная с pg 11,
         * но это применимо только к первоначальному определению таблицы, просто так поальтерить индекс нельзя
         */
        private boolean isPreferredOver(Index other) {
            if (isPartial) {
                if (other.isPartial) {
                    // TODO проверить одинаковые ли у них условия, и только потом сравнивать длину name
                    return name.compareTo(other.name) <= 0;
                }
                // частичные индексы не могут заменять
                return false;
            }
            if (priority() > other.priority()) {
                return length() >= other.length();
            } else if (priority() == other.priority()) {
                if (length() > other.length()) {
                    return true;
                } else if (length() == other.length()) {
                    return name.compareTo(other.name) <= 0;
                }
            }
            return false;
        }

        private int priority() {
            return isPrimary
                    ? 2
                    : isUnique
                    ? 1
                    : 0;
        }

        private boolean canBeConvertedToPk() {
            return !isPrimary
                    && isUnique
                    && areIndexColsNotNull
                    && !isFunctional
                    && !columns.isEmpty();
        }

    }

    private enum ConstraintType implements HasId<String> {
        PRIMARY_KEY("primary key"),
        CHECK("check"),
        INDEX("index"),
        FOREIGN_KEY("foreign key");

        private final String id;

        ConstraintType(String id) {
            this.id = id;
        }

        @Override
        public String getId() {
            return id;
        }

    }

    private class Constraint implements Runnable {
        final String schemaName;
        final String tableName;
        final String constraintName;
        final ConstraintType constraintType;
        final Set<String> columns;

        Constraint(
                String schemaName,
                String tableName,
                String constraintName,
                String constraintType,
                Set<String> columns
        ) {
            this.schemaName = schemaName;
            this.tableName = tableName;
            this.constraintName = constraintName;
            this.columns = columns;
            this.constraintType = HasId.getById(ConstraintType.class, constraintType.toLowerCase());
        }

        @Override
        public void run() {
            validateConstraintName(ConstraintType.PRIMARY_KEY, "pk", WRONG_PRIMARY_KEY_NAME);
            validateConstraintName(ConstraintType.INDEX, "i", WRONG_INDEX_NAME);
            validateConstraintName(ConstraintType.CHECK, "c", WRONG_CHECK_NAME);
        }

        void validateConstraintName(
                ConstraintType checkedType,
                String desiredPrefix,
                String nameError
        ) {
            if (constraintType != checkedType) {
                return; // poor's man oop
            }
            var letters = 3;
            String regex;
            String suggestedName;
            do {
                regex = createRegexpWithPrefix(desiredPrefix, letters);
                suggestedName = makeNameFromRegex(regex);
                letters--;
            } while (letters > 0 && suggestedName.length() > PG_OBJECT_NAME_LIMIT);
            if (!isValidConstraintName(regex, letters + 1)) {
                errors.add(String.format(
                        nameError,
                        constraintName,
                        schemaName,
                        tableName,
                        suggestedName
                ));
            }
        }

        String createRegexpWithPrefix(String prefix, int letters) {
            return prefix + "_" + createMainRegexpPart(
                    schemaName,
                    tableName,
                    columns,
                    letters
            );
        }

        String createMainRegexpPart(
                String schemaName,
                String tableName,
                Set<String> columns,
                int letters
        ) {
            var regexp = new StringBuilder();

            // optional schema names, only for multiple schemas db
            if (schemasCount > 1) {
                regexp.append("(?:");
                appendWordParts(schemaName, letters, regexp);
                regexp.append("_");
                regexp.append(")?");
            }

            // table name
            appendWordParts(tableName, letters, regexp);

            // column names in any order
            if (!columns.isEmpty()) {
                regexp.append("(?:");
                for (String column : columns) {
                    regexp.append("_");
                    appendWordParts(column, letters, regexp);
                    regexp.append("|");
                }
                regexp.deleteCharAt(regexp.length() - 1); // remove trailing "|"
                regexp.append("){").append(columns.size()).append("}");
            }

            return regexp.toString();
        }

        private void appendWordParts(String objectName, int letters, StringBuilder regexp) {
            for (var word : objectName.split("_")) {
                regexp.append(getFirstNLettersOrLess(word, letters)).append("[a-z0-9]*");
            }
        }

        boolean isValidConstraintName(String regex, int letters) {
            return constraintName.matches(regex)
                    && columns.stream()
                    .distinct()
                    .allMatch(columnName -> Stream.of(columnName.split("_"))
                            .allMatch(word -> constraintName.contains(getFirstNLettersOrLess(word, letters))));
        }
    }

    private final class ForeignKey extends Constraint {
        private final String refSchemaName;
        private final String refTableName;
        private final Set<String> refColumnNames;

        ForeignKey(
                String schemaName,
                String tableName,
                String refSchemaName,
                String refTableName,
                String constraintName,
                Set<String> refColumnNames,
                Set<String> columns
        ) {
            super(schemaName, tableName, constraintName, "foreign key", columns);
            this.refSchemaName = refSchemaName;
            this.refTableName = refTableName;
            this.refColumnNames = refColumnNames;
        }

        @Override
        public void run() {
            validateConstraintName(ConstraintType.FOREIGN_KEY, "fk", WRONG_FOREIGN_KEY_NAME);
        }

        @Override
        String createRegexpWithPrefix(
                String prefix,
                int letters
        ) {
            return super.createRegexpWithPrefix(prefix, letters) + "_" + createMainRegexpPart(
                    refSchemaName,
                    refTableName,
                    refColumnNames,
                    letters
            );
        }

        @Override
        boolean isValidConstraintName(String regex, int letters) {
            return super.isValidConstraintName(regex, letters)
                    && refColumnNames.stream()
                    .distinct()
                    .allMatch(columnName -> Stream.of(columnName.split("_"))
                            .allMatch(word -> constraintName.contains(getFirstNLettersOrLess(word, letters))));
        }
    }
}
